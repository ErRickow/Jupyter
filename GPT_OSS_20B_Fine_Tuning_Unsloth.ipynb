{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ðŸ¦¥ Fine-Tuning GPT-OSS-20B dengan Unsloth di Google Colab\n",
    "\n",
    "Notebook ini akan memandu Anda untuk fine-tune model **GPT-OSS-20B** menggunakan **Unsloth** - library yang mengoptimalkan training hingga **1.5x lebih cepat** dengan **70% lebih sedikit VRAM**.\n",
    "\n",
    "## ðŸ“‹ Yang Akan Dipelajari:\n",
    "1. âœ… Setup environment dan instalasi dependencies\n",
    "2. âœ… Load model GPT-OSS-20B dengan 4-bit quantization\n",
    "3. âœ… Konfigurasi LoRA adapters untuk parameter-efficient fine-tuning\n",
    "4. âœ… Prepare dataset untuk training\n",
    "5. âœ… Training dengan SFTTrainer\n",
    "6. âœ… Inference dan testing model\n",
    "7. âœ… Save dan export model\n",
    "\n",
    "## ðŸŽ¯ Spesifikasi:\n",
    "- **Model**: OpenAI GPT-OSS-20B (21B parameters, 3.6B active)\n",
    "- **Method**: QLoRA (Quantized Low-Rank Adaptation)\n",
    "- **VRAM Required**: ~14GB (cocok untuk Colab T4/L4)\n",
    "- **Training Speed**: 1.5x faster vs standard implementation\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¡ Tips**: Pastikan menggunakan GPU runtime di Colab:\n",
    "- Runtime â†’ Change runtime type â†’ T4 GPU / L4 GPU (recommended)\n",
    "\n",
    "Mari kita mulai! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "installation"
   },
   "source": [
    "## 1ï¸âƒ£ Setup & Instalasi\n",
    "\n",
    "Kita akan install Unsloth dan dependencies yang diperlukan. Proses ini memakan waktu ~3-5 menit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install uv package manager untuk instalasi yang lebih cepat\n",
    "!pip install --upgrade -qqq uv\n",
    "\n",
    "# Install Unsloth dan dependencies\n",
    "# PENTING: Gunakan versi torch>=2.8.0 dan triton>=3.4.0\n",
    "!uv pip install -qqq \\\n",
    "    \"torch>=2.8.0\" \"triton>=3.4.0\" numpy \\\n",
    "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "    torchvision bitsandbytes \"transformers>=4.55.3\" \\\n",
    "    datasets trl peft accelerate\n",
    "\n",
    "print(\"âœ… Instalasi selesai!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_gpu"
   },
   "source": [
    "### Verifikasi GPU Setup\n",
    "\n",
    "Pastikan kita memiliki GPU dengan memory yang cukup (minimal 14GB untuk QLoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Cek GPU yang tersedia\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 2)\n",
    "    print(f\"ðŸŽ® GPU: {gpu_stats.name}\")\n",
    "    print(f\"ðŸ’¾ Total Memory: {max_memory} GB\")\n",
    "    print(f\"ðŸ”§ CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    if max_memory >= 14:\n",
    "        print(f\"\\nâœ… Memory cukup untuk QLoRA training!\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ Memory mungkin kurang. Recommended: 14GB+ untuk GPT-OSS-20B\")\n",
    "else:\n",
    "    print(\"âŒ GPU tidak terdeteksi! Pastikan runtime menggunakan GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_loading"
   },
   "source": [
    "## 2ï¸âƒ£ Load Model GPT-OSS-20B\n",
    "\n",
    "Kita akan load model dengan:\n",
    "- **4-bit quantization** untuk menghemat VRAM\n",
    "- **Max sequence length 2048** tokens\n",
    "- Model dari Unsloth yang sudah dioptimasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_model"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Konfigurasi model\n",
    "max_seq_length = 2048  # Bisa dinaikkan untuk context yang lebih panjang\n",
    "dtype = None  # Auto-detect: Float16 untuk Tesla T4/V100, Bfloat16 untuk Ampere+\n",
    "load_in_4bit = True  # Gunakan 4-bit quantization untuk hemat VRAM\n",
    "\n",
    "print(\"ðŸ“¥ Loading GPT-OSS-20B model...\")\n",
    "print(\"â³ Proses ini memakan waktu 2-3 menit...\\n\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/gpt-oss-20b\",  # Model yang sudah dioptimasi Unsloth\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    full_finetuning=False,  # Gunakan LoRA, bukan full fine-tuning\n",
    ")\n",
    "\n",
    "print(\"âœ… Model berhasil dimuat!\")\n",
    "print(f\"ðŸ“Š Model dtype: {model.dtype}\")\n",
    "print(f\"ðŸ“ Max sequence length: {max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lora_config"
   },
   "source": [
    "## 3ï¸âƒ£ Konfigurasi LoRA Adapters\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** memungkinkan kita fine-tune hanya ~1% parameters, menghemat memory dan waktu training secara signifikan.\n",
    "\n",
    "### Parameter Explanation:\n",
    "- **r=8**: LoRA rank (higher = lebih expressive, tapi lebih lambat)\n",
    "- **lora_alpha=16**: Scaling factor untuk LoRA weights\n",
    "- **lora_dropout=0**: Dropout rate (0 = no dropout)\n",
    "- **target_modules**: Layer mana yang akan di-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_lora"
   },
   "outputs": [],
   "source": [
    "# Tambahkan LoRA adapters ke model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,  # LoRA rank - bisa dinaikkan untuk hasil lebih baik (8, 16, 32, 64)\n",
    "    target_modules=[\n",
    "        \"q_proj\",    # Query projection\n",
    "        \"k_proj\",    # Key projection\n",
    "        \"v_proj\",    # Value projection\n",
    "        \"o_proj\",    # Output projection\n",
    "        \"gate_proj\", # Gate projection (MoE)\n",
    "        \"up_proj\",   # Up projection (FFN)\n",
    "        \"down_proj\", # Down projection (FFN)\n",
    "    ],\n",
    "    lora_alpha=16,  # Scaling factor\n",
    "    lora_dropout=0,  # Dropout untuk regularization (0 = disabled)\n",
    "    bias=\"none\",  # Tidak train bias parameters\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Hemat memory dengan gradient checkpointing\n",
    "    random_state=3407,  # Random seed untuk reproducibility\n",
    ")\n",
    "\n",
    "print(\"âœ… LoRA adapters berhasil ditambahkan!\")\n",
    "print(\"\\nðŸ“Š Trainable parameters:\")\n",
    "\n",
    "# Hitung berapa parameter yang akan di-train\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / all_params\n",
    "\n",
    "print(f\"Trainable: {trainable_params:,} ({trainable_percent:.2f}%)\")\n",
    "print(f\"Total: {all_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset_section"
   },
   "source": [
    "## 4ï¸âƒ£ Prepare Dataset\n",
    "\n",
    "Kita akan menggunakan dataset publik untuk contoh. Anda bisa mengganti dengan dataset Anda sendiri.\n",
    "\n",
    "### Format Dataset:\n",
    "Dataset harus dalam format **ShareGPT/chat** dengan struktur:\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Pertanyaan...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Jawaban...\"}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "# Load dataset - contoh: multilingual reasoning dataset\n",
    "# GANTI dengan dataset Anda sendiri jika perlu\n",
    "print(\"ðŸ“¥ Loading dataset...\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"HuggingFaceH4/Multilingual-Thinking\",  # Dataset example\n",
    "    split=\"train[:1000]\"  # Ambil 1000 samples pertama untuk demo\n",
    ")\n",
    "\n",
    "# Standardisasi format ShareGPT\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "print(f\"âœ… Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nðŸ“ Contoh data:\")\n",
    "print(dataset[0][\"messages\"][:2])  # Tampilkan 2 messages pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format_dataset"
   },
   "source": [
    "### Format Dataset untuk Training\n",
    "\n",
    "Kita perlu convert messages ke format text menggunakan chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prepare_dataset"
   },
   "outputs": [],
   "source": [
    "# Fungsi untuk format prompts\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Convert messages ke text format menggunakan chat template\"\"\"\n",
    "    convos = examples[\"messages\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "print(\"ðŸ”„ Formatting dataset...\")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names  # Remove original columns\n",
    ")\n",
    "\n",
    "print(\"âœ… Dataset ready untuk training!\")\n",
    "print(f\"\\nðŸ“„ Contoh formatted text (100 chars):\")\n",
    "print(dataset[0][\"text\"][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_dataset"
   },
   "source": [
    "### ðŸ’¡ Cara Menggunakan Dataset Sendiri\n",
    "\n",
    "Jika Anda ingin menggunakan dataset custom, gunakan kode ini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_data_example"
   },
   "outputs": [],
   "source": [
    "# CONTOH: Membuat dataset custom\n",
    "# Uncomment dan modifikasi sesuai kebutuhan\n",
    "\n",
    "# from datasets import Dataset\n",
    "\n",
    "# # Data Anda dalam format list of dicts\n",
    "# custom_data = [\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             {\"role\": \"user\", \"content\": \"Apa itu Python?\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Python adalah bahasa pemrograman...\"}\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             {\"role\": \"user\", \"content\": \"Bagaimana cara membuat function?\"},\n",
    "#             {\"role\": \"assistant\", \"content\": \"Gunakan keyword 'def' untuk...\"}\n",
    "#         ]\n",
    "#     },\n",
    "#     # Tambahkan lebih banyak data...\n",
    "# ]\n",
    "\n",
    "# # Convert ke Hugging Face Dataset\n",
    "# dataset = Dataset.from_list(custom_data)\n",
    "# dataset = standardize_sharegpt(dataset)\n",
    "# dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# print(f\"âœ… Custom dataset ready: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training_section"
   },
   "source": [
    "## 5ï¸âƒ£ Training Configuration\n",
    "\n",
    "Setup trainer dengan hyperparameters yang optimal untuk GPT-OSS-20B.\n",
    "\n",
    "### Hyperparameters Explanation:\n",
    "- **per_device_train_batch_size**: Jumlah samples per batch (turunkan jika OOM)\n",
    "- **gradient_accumulation_steps**: Akumulasi gradients untuk batch size efektif yang lebih besar\n",
    "- **learning_rate**: Learning rate (2e-4 optimal untuk LoRA)\n",
    "- **max_steps**: Total training steps (set -1 untuk full epoch)\n",
    "- **optim**: Optimizer (adamw_8bit hemat memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_trainer"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training configuration\n",
    "training_args = SFTConfig(\n",
    "    # Batch settings\n",
    "    per_device_train_batch_size=1,  # Batch size per GPU (turunkan jika OOM)\n",
    "    gradient_accumulation_steps=4,  # Effective batch size = 1 * 4 = 4\n",
    "    \n",
    "    # Learning rate & scheduler\n",
    "    learning_rate=2e-4,  # Learning rate untuk LoRA\n",
    "    lr_scheduler_type=\"linear\",  # Linear decay\n",
    "    warmup_steps=5,  # Warmup steps\n",
    "    \n",
    "    # Training steps\n",
    "    max_steps=60,  # Total steps (set -1 untuk full epoch)\n",
    "    num_train_epochs=1,  # Jumlah epoch (jika max_steps=-1)\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"adamw_8bit\",  # 8-bit optimizer untuk hemat memory\n",
    "    weight_decay=0.01,  # Weight decay untuk regularization\n",
    "    \n",
    "    # Logging & saving\n",
    "    logging_steps=1,  # Log setiap step\n",
    "    output_dir=\"outputs\",  # Output directory\n",
    "    save_strategy=\"steps\",  # Save strategy\n",
    "    save_steps=20,  # Save setiap 20 steps\n",
    "    \n",
    "    # Precision\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 jika GPU tidak support BF16\n",
    "    bf16=torch.cuda.is_bf16_supported(),  # BF16 jika GPU support\n",
    "    \n",
    "    # Misc\n",
    "    seed=3407,  # Random seed\n",
    "    dataset_text_field=\"text\",  # Field yang berisi text\n",
    "    max_seq_length=max_seq_length,  # Max sequence length\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer configured!\")\n",
    "print(f\"\\nðŸ“Š Training settings:\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Max steps: {training_args.max_steps}\")\n",
    "print(f\"  - Precision: {'BF16' if training_args.bf16 else 'FP16'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start_training"
   },
   "source": [
    "### ðŸš€ Start Training!\n",
    "\n",
    "**PERHATIAN**: Training akan memakan waktu tergantung dataset size dan max_steps.\n",
    "- 60 steps dengan batch size 4: ~5-10 menit di T4/L4\n",
    "- Monitor loss - seharusnya turun secara konsisten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Starting training...\\n\")\n",
    "print(\"â° Ini akan memakan waktu beberapa menit...\")\n",
    "print(\"ðŸ“Š Monitor loss di bawah - seharusnya turun secara bertahap\\n\")\n",
    "\n",
    "# Start training\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸŽ‰ TRAINING SELESAI!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nðŸ“ˆ Training Statistics:\")\n",
    "print(f\"  - Final loss: {trainer_stats.metrics['train_loss']:.4f}\")\n",
    "print(f\"  - Total steps: {trainer_stats.global_step}\")\n",
    "print(f\"  - Training time: {trainer_stats.metrics.get('train_runtime', 0):.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference_section"
   },
   "source": [
    "## 6ï¸âƒ£ Testing & Inference\n",
    "\n",
    "Sekarang mari test model yang sudah di-fine-tune!\n",
    "\n",
    "GPT-OSS mendukung **reasoning effort levels**:\n",
    "- `low`: Fast inference, simple reasoning\n",
    "- `medium`: Balanced (default)\n",
    "- `high`: Slow but thorough reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_inference"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Enable inference mode untuk model\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Fungsi helper untuk inference\n",
    "def generate_response(prompt, system_message=\"You are a helpful assistant.\", reasoning_effort=\"medium\"):\n",
    "    \"\"\"\n",
    "    Generate response dari model\n",
    "    \n",
    "    Args:\n",
    "        prompt: User input/question\n",
    "        system_message: System prompt\n",
    "        reasoning_effort: 'low', 'medium', atau 'high'\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_dict=True,\n",
    "        reasoning_effort=reasoning_effort,  # Set reasoning level\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate dengan streaming output\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,  # Max tokens untuk response\n",
    "        temperature=0.7,  # Temperature untuk sampling (lower = lebih deterministic)\n",
    "        top_p=0.9,  # Nucleus sampling\n",
    "        streamer=text_streamer,  # Stream output ke console\n",
    "        use_cache=True,  # Use KV cache untuk faster generation\n",
    "    )\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "print(\"âœ… Inference function ready!\")\n",
    "print(\"\\nðŸ’¡ Gunakan: generate_response('Your question here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_examples"
   },
   "source": [
    "### Test dengan Beberapa Contoh\n",
    "\n",
    "Mari coba model dengan berbagai jenis pertanyaan!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_1"
   },
   "outputs": [],
   "source": [
    "# Test 1: Simple question\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ§ª TEST 1: Simple Math Problem\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nðŸ’¬ User: Solve x^2 + 5x + 6 = 0\\n\")\n",
    "print(\"ðŸ¤– Assistant:\\n\")\n",
    "\n",
    "generate_response(\n",
    "    \"Solve x^2 + 5x + 6 = 0\",\n",
    "    reasoning_effort=\"medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_2"
   },
   "outputs": [],
   "source": [
    "# Test 2: Coding question\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ§ª TEST 2: Coding Question\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nðŸ’¬ User: Write a Python function to check if a number is prime\\n\")\n",
    "print(\"ðŸ¤– Assistant:\\n\")\n",
    "\n",
    "generate_response(\n",
    "    \"Write a Python function to check if a number is prime\",\n",
    "    system_message=\"You are an expert Python programmer.\",\n",
    "    reasoning_effort=\"low\"  # Low untuk code generation yang simple\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "example_3"
   },
   "outputs": [],
   "source": [
    "# Test 3: Complex reasoning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ§ª TEST 3: Complex Reasoning\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nðŸ’¬ User: Explain the halting problem and why it's undecidable\\n\")\n",
    "print(\"ðŸ¤– Assistant:\\n\")\n",
    "\n",
    "generate_response(\n",
    "    \"Explain the halting problem and why it's undecidable\",\n",
    "    reasoning_effort=\"high\"  # High untuk reasoning yang kompleks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_test"
   },
   "source": [
    "### ðŸ’­ Test dengan Pertanyaan Anda Sendiri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "user_test"
   },
   "outputs": [],
   "source": [
    "# Ganti dengan pertanyaan Anda!\n",
    "your_question = \"What is the difference between AI and Machine Learning?\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ðŸ§ª YOUR TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ’¬ User: {your_question}\\n\")\n",
    "print(\"ðŸ¤– Assistant:\\n\")\n",
    "\n",
    "generate_response(\n",
    "    your_question,\n",
    "    reasoning_effort=\"medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_section"
   },
   "source": [
    "## 7ï¸âƒ£ Save & Export Model\n",
    "\n",
    "Ada beberapa cara untuk save model:\n",
    "1. **LoRA adapters only** - Paling kecil (beberapa MB)\n",
    "2. **Merged model** - Full model dengan adapters merged (16-bit)\n",
    "3. **Quantized (GGUF)** - Untuk deployment (4-bit/8-bit)\n",
    "4. **Push to Hugging Face Hub** - Share dengan komunitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_lora"
   },
   "outputs": [],
   "source": [
    "# Option 1: Save LoRA adapters only (recommended untuk iterasi cepat)\n",
    "print(\"ðŸ’¾ Saving LoRA adapters...\")\n",
    "\n",
    "lora_output_dir = \"gpt-oss-20b-lora-adapters\"\n",
    "model.save_pretrained(lora_output_dir)\n",
    "tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "print(f\"âœ… LoRA adapters saved to: {lora_output_dir}\")\n",
    "print(\"ðŸ“¦ Size: ~10-50MB (hanya adapters, bukan full model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_merged"
   },
   "outputs": [],
   "source": [
    "# Option 2: Save merged model (16-bit)\n",
    "# WARNING: Ini akan menggunakan banyak disk space (~40GB)\n",
    "\n",
    "print(\"ðŸ’¾ Saving merged model (16-bit)...\")\n",
    "print(\"âš ï¸  Ini memerlukan ~40GB disk space\\n\")\n",
    "\n",
    "merged_output_dir = \"gpt-oss-20b-finetuned-merged\"\n",
    "\n",
    "model.save_pretrained_merged(\n",
    "    merged_output_dir,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",  # Atau \"merged_4bit\" untuk quantized\n",
    ")\n",
    "\n",
    "print(f\"âœ… Merged model saved to: {merged_output_dir}\")\n",
    "print(\"ðŸ“¦ Size: ~40GB (full model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_gguf"
   },
   "outputs": [],
   "source": [
    "# Option 3: Export to GGUF untuk llama.cpp\n",
    "# Berguna untuk deployment di CPU atau mobile devices\n",
    "\n",
    "print(\"ðŸ’¾ Exporting to GGUF format...\")\n",
    "\n",
    "# Quantization methods: \"q4_k_m\", \"q5_k_m\", \"q8_0\", \"f16\"\n",
    "quantization_method = \"q4_k_m\"  # 4-bit quantization (recommended)\n",
    "\n",
    "model.save_pretrained_gguf(\n",
    "    \"gpt-oss-20b-finetuned\",  # Folder name\n",
    "    tokenizer,\n",
    "    quantization_method=quantization_method,\n",
    ")\n",
    "\n",
    "print(f\"âœ… GGUF model exported dengan {quantization_method} quantization\")\n",
    "print(\"ðŸ“¦ Size: ~10-15GB (4-bit quantized)\")\n",
    "print(\"\\nðŸ’¡ File ini bisa digunakan dengan llama.cpp atau Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "push_to_hub"
   },
   "outputs": [],
   "source": [
    "# Option 4: Push to Hugging Face Hub\n",
    "# Anda perlu login ke Hugging Face terlebih dahulu\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login ke Hugging Face (uncomment untuk menggunakan)\n",
    "# hf_token = \"hf_...\"  # Ganti dengan token Anda dari https://huggingface.co/settings/tokens\n",
    "# login(token=hf_token)\n",
    "\n",
    "# # Push LoRA adapters\n",
    "# repo_name = \"your-username/gpt-oss-20b-finetuned\"  # Ganti dengan nama repo Anda\n",
    "# print(f\"ðŸš€ Pushing to Hugging Face Hub: {repo_name}\")\n",
    "\n",
    "# model.push_to_hub(\n",
    "#     repo_name,\n",
    "#     token=hf_token,\n",
    "#     private=False,  # Set True untuk private repo\n",
    "# )\n",
    "# tokenizer.push_to_hub(repo_name, token=hf_token)\n",
    "\n",
    "# print(f\"âœ… Model pushed to: https://huggingface.co/{repo_name}\")\n",
    "\n",
    "print(\"ðŸ’¡ Uncomment code di atas untuk push ke Hugging Face Hub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_saved"
   },
   "source": [
    "### ðŸ”„ Cara Load Model yang Sudah Disave\n",
    "\n",
    "Untuk menggunakan model yang sudah di-save nanti:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "reload_example"
   },
   "outputs": [],
   "source": [
    "# Contoh: Load LoRA adapters\n",
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"gpt-oss-20b-lora-adapters\",  # Path ke saved adapters\n",
    "#     max_seq_length=2048,\n",
    "#     dtype=None,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "\n",
    "# # Enable inference\n",
    "# FastLanguageModel.for_inference(model)\n",
    "\n",
    "# # Langsung bisa digunakan!\n",
    "# generate_response(\"Your question here\")\n",
    "\n",
    "print(\"ðŸ’¡ Uncomment code di atas untuk reload saved model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ðŸŽ“ Kesimpulan & Next Steps\n",
    "\n",
    "Selamat! Anda telah berhasil:\n",
    "âœ… Setup environment dengan Unsloth  \n",
    "âœ… Load GPT-OSS-20B model dengan 4-bit quantization  \n",
    "âœ… Configure LoRA adapters untuk efficient training  \n",
    "âœ… Fine-tune model dengan dataset custom  \n",
    "âœ… Test model dengan berbagai reasoning levels  \n",
    "âœ… Save dan export model dalam berbagai format  \n",
    "\n",
    "### ðŸ“š Sumber Belajar Lebih Lanjut:\n",
    "- [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "- [GPT-OSS GitHub](https://github.com/openai/gpt-oss)\n",
    "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
    "\n",
    "### ðŸš€ Next Steps:\n",
    "1. **Experiment dengan hyperparameters** - Coba learning rate, LoRA rank, dll\n",
    "2. **Gunakan dataset lebih besar** - Lebih banyak data = hasil lebih baik\n",
    "3. **Train lebih lama** - Tingkatkan max_steps atau epochs\n",
    "4. **Try different tasks** - Fine-tune untuk specific domain (coding, medical, dll)\n",
    "5. **Deploy model** - Gunakan llama.cpp, vLLM, atau TGI untuk production\n",
    "\n",
    "### âš¡ Tips untuk Hasil Optimal:\n",
    "- **Dataset quality > quantity** - Fokus pada data berkualitas tinggi\n",
    "- **Monitor loss curve** - Loss harus turun smooth, tidak erratic\n",
    "- **Use validation set** - Split data untuk tracking overfitting\n",
    "- **Experiment dengan LoRA rank** - Higher rank = lebih expressive tapi slower\n",
    "- **Try different reasoning efforts** - Match dengan use case Anda\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ¦¥ Powered by Unsloth** - 1.5x faster, 70% less VRAM\n",
    "\n",
    "**ðŸ“ Created**: 2025 | **License**: Apache 2.0\n",
    "\n",
    "Jika ada pertanyaan, silakan cek dokumentasi atau buka issue di GitHub!\n",
    "\n",
    "Happy fine-tuning! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## ðŸ”§ Troubleshooting\n",
    "\n",
    "### âŒ Out of Memory (OOM)\n",
    "**Solusi:**\n",
    "```python\n",
    "# 1. Turunkan batch size\n",
    "per_device_train_batch_size = 1\n",
    "\n",
    "# 2. Turunkan max_seq_length\n",
    "max_seq_length = 1024  # dari 2048\n",
    "\n",
    "# 3. Gunakan gradient checkpointing\n",
    "use_gradient_checkpointing = \"unsloth\"\n",
    "\n",
    "# 4. Turunkan LoRA rank\n",
    "r = 4  # dari 8\n",
    "```\n",
    "\n",
    "### âš ï¸ Loss tidak turun\n",
    "**Solusi:**\n",
    "```python\n",
    "# 1. Cek learning rate - mungkin terlalu kecil/besar\n",
    "learning_rate = 1e-4  # Coba berbagai nilai: 1e-5, 5e-5, 1e-4, 2e-4\n",
    "\n",
    "# 2. Tingkatkan warmup steps\n",
    "warmup_steps = 10\n",
    "\n",
    "# 3. Cek dataset - pastikan formatnya benar\n",
    "print(dataset[0]['text'][:500])\n",
    "```\n",
    "\n",
    "### ðŸŒ Training terlalu lambat\n",
    "**Solusi:**\n",
    "```python\n",
    "# 1. Gunakan GPU yang lebih cepat (L4 > T4)\n",
    "# 2. Tingkatkan batch size jika memory cukup\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# 3. Turunkan max_seq_length jika tidak butuh context panjang\n",
    "max_seq_length = 1024\n",
    "```\n",
    "\n",
    "### ðŸ”„ Model output tidak bagus\n",
    "**Solusi:**\n",
    "```python\n",
    "# 1. Train lebih lama\n",
    "max_steps = 200  # atau lebih\n",
    "\n",
    "# 2. Gunakan dataset lebih besar dan berkualitas\n",
    "# 3. Tingkatkan LoRA rank\n",
    "r = 16  # atau 32\n",
    "\n",
    "# 4. Coba full fine-tuning (butuh lebih banyak VRAM)\n",
    "full_finetuning = True\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
